
ordered param，贪婪版本，就是用一阶导数除以二阶导数的那种。然后去笔记本里面测试一下再决定。

indicator的结果要和GBN的scale联系起来。需要一个公式。




最近的新东西，make linear，应该还是有用的。

sparser基本可以确定，在没有用和很难用之间。
现在就等新的linear了。
然后做一个新的module容器。



x:
h1 = w1(x+1)+b1
h2 = w2(x-1)+b2
x = h1+h2
return x
Thus, no BN is needed.



Also, is w is originally 	quadratic . 
Not wx+b, it's wwx+bb, what would the gradient look like.



super linear layer.



5
[basically DONE]indicator to show if some param goes to a direction or shake around a point.




1
g dropout( to be tested) probably doesn't help at all.

2[probably DONE]
make linear WIP (util.py) w should be rand/sqrt(dim_in), or normalize it(length to 1), and bias should be a const for every output.




3WIP=================
bn cont
resnet cont. Also let it contain the selmul layer.

4
[x,y]+b1)*w  and then + b2. The first b1 should have -1 0 and 1.
Like:
input is [x,y] or anything similar to this. Small, ranges -1 to 1 or 0 to 1 or so.
x = x.repeat(1,1,1,1....,3) only 3 for the last dim.
x+[-1, -1, 0, 0, 1, 1]
then x = self.Lin0(x)
This small b1 can collapse into some usual nn structure without training.
Also some other methods to handle this.


6
3 order and 5 order parameters. Then a linear layer with this. Then test it.
The new linear layer from plain torch linear layer.

instead of wx+b, replace the w with something like w*w or w3 or aw+bw2+cw3. It may provide the secondary derivative, or
even the 3rd order derivative. This may help train and converge faster.
