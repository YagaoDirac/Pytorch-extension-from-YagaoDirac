super linear layer.



5
[DONE]indicator to show if some param goes to a direction or shake around a point.




1
g dropout( to be tested) probably doesn't help at all.

2[DONE]
make linear WIP (util.py) w should be rand/sqrt(dim_in), or normalize it(length to 1), and bias should be a const for every output.

WIP


3
bn cont
resnet cont. Also let it contain the selmul layer.

4
[x,y]+b1)*w  and then + b2. The first b1 should have -1 0 and 1.
Like:
input is [x,y] or anything similar to this. Small, ranges -1 to 1 or 0 to 1 or so.
x = x.repeat(1,1,1,1....,3) only 3 for the last dim.
x+[-1, -1, 0, 0, 1, 1]
then x = self.Lin0(x)
This small b1 can collapse into some usual nn structure without training.
Also some other methods to handle this.


6
3 order and 5 order parameters. Then a linear layer with this. Then test it.
The new linear layer from plain torch linear layer.

instead of wx+b, replace the w with something like w*w or w3 or aw+bw2+cw3. It may provide the secondary derivative, or
even the 3rd order derivative. This may help train and converge faster.
